{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# работа с датафреймами и numpy-массивами\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# подготовка данных к обучению\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# обучение модели\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# оценка точности модели\n",
    "from sklearn import metrics\n",
    "\n",
    "# удаление стоп-слов\n",
    "import nltk\n",
    "\n",
    "# лемматизация\n",
    "import pymorphy2\n",
    "\n",
    "# Отрисовка графиков\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Регулярные выражения\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def lemmatize(text):\n",
    "    res = list()\n",
    "    for word in text:\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.normal_form)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(string):\n",
    "    # Приводим буквы к нижнему регистру\n",
    "    string = string.lower()\n",
    "    # Убираем ссылки \n",
    "    string = re.sub(r\"http\\S+\", \"\", string)\n",
    "    # Убираем символы переноса строки\n",
    "    string = string.replace('\\\\n', '')\n",
    "    string = string.replace('\\\\', '')\n",
    "    # Заменяем букву \"ё\" на \"е\"\n",
    "    string = str.replace(string,'ё','е')\n",
    "    # Ищем все буквенные конструкции и представляем их в виде токенов\n",
    "    prog = re.compile('[А-Яа-яA-Za-z]+')\n",
    "    words = prog.findall(string)\n",
    "    # Удаляем стоп-слова\n",
    "    stopwords = nltk.corpus.stopwords.words('russian')\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    ## Лемматизация и представление массива слов в виде строки\n",
    "    resultString = ' '.join(lemmatize(words))\n",
    "    return resultString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка точности на одном наборе данных с помощью сплита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_with_split(text, target):\n",
    "    # Обработка текста\n",
    "    text = list(map(cleanText, text))\n",
    "    # Разбиение выборки на обучающую и тестовую\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text, target, test_size=.3, stratify=target, shuffle = True, random_state=0)\n",
    "    # Обучение модели\n",
    "    clf_pipeline = Pipeline(\n",
    "            [(\"vectorizer\", TfidfVectorizer()), # Prod feature: tokenizer=cleanText\n",
    "            (\"classifier\", LinearSVC())]\n",
    "        )\n",
    "    clf_pipeline.fit(X_train, y_train)\n",
    "    # Предсказание\n",
    "    predict = clf_pipeline.predict(X_test)\n",
    "    # Оценка точности \n",
    "    f1_base = metrics.f1_score(y_test, predict)\n",
    "    return f1_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка точности с помощью двух наборов данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed(train_text, train_target, test_text, test_target):\n",
    "    # Обработка текста\n",
    "    train_text = list(map(cleanText, train_text))\n",
    "    test_text = list(map(cleanText, test_text))\n",
    "    # Обучение модели\n",
    "    clf_pipeline = Pipeline(\n",
    "            [(\"vectorizer\", TfidfVectorizer()), # Prod feature: tokenizer=cleanText\n",
    "            (\"classifier\", LinearSVC())]\n",
    "        )\n",
    "    clf_pipeline.fit(train_text, train_target)\n",
    "    # Предсказание\n",
    "    predict = clf_pipeline.predict(test_text)\n",
    "    # Оценка точности \n",
    "    f1_base = metrics.f1_score(test_target, predict)\n",
    "    return f1_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рассчет точности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка наборов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор с Подслушано"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество деструктивных комментариев: 1978\n",
      "Количество недеструктивных комментариев: 11144\n"
     ]
    }
   ],
   "source": [
    "podslushano_labeled = pd.read_csv('podslushano_labeled.csv')\n",
    "podslushano_labeled = podslushano_labeled.drop('Unnamed: 0', axis=1)\n",
    "podslushano_labeled = podslushano_labeled.fillna(0)\n",
    "text_podslushano = np.array(podslushano_labeled['Text'].values)\n",
    "target_podslushano = podslushano_labeled['Target'].astype(int).values\n",
    "print('Количество деструктивных комментариев:', len(podslushano_labeled[podslushano_labeled['Target'] == 1]))\n",
    "print('Количество недеструктивных комментариев:', len(podslushano_labeled[podslushano_labeled['Target'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор с Пикабу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество деструктивных комментариев: 4826\n",
      "Количество недеструктивных комментариев: 9586\n"
     ]
    }
   ],
   "source": [
    "pikabu_labeled = pd.read_csv('labeled.csv')\n",
    "text_pikabu = np.array(pikabu_labeled['comment'].values)\n",
    "target_pikabu = pikabu_labeled['toxic'].astype(int).values\n",
    "print('Количество деструктивных комментариев:', len(pikabu_labeled[pikabu_labeled['toxic'] == 1]))\n",
    "print('Количество недеструктивных комментариев:', len(pikabu_labeled[pikabu_labeled['toxic'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка точности классификации набора подслушано (модель обучена на наборе подслушано)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4587737843551797\n"
     ]
    }
   ],
   "source": [
    "print(processed_with_split(text_podslushano, target_podslushano))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка точности классификации набора пикабу (модель обучена на наборе подслушано)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39788787078738935\n"
     ]
    }
   ],
   "source": [
    "print(processed(text_podslushano, target_podslushano, text_pikabu, target_pikabu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка точности классификации набора пикабу (модель обучена на наборе пикабу)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7955307262569832\n"
     ]
    }
   ],
   "source": [
    "print(processed_with_split(text_pikabu, target_pikabu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка точности классификации набора подслушано (модель обучена на наборе пикубу)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2619867549668874\n"
     ]
    }
   ],
   "source": [
    "print(processed(text_pikabu, target_pikabu, text_podslushano, target_podslushano))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка точности на объединенных наборах послушано и пикабу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6169471851421938\n"
     ]
    }
   ],
   "source": [
    "print(processed_with_split(np.append(text_pikabu, text_podslushano), np.append(target_pikabu, target_podslushano)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чистка набора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balansing_for_word(df, word):\n",
    "    i = 0\n",
    "    bad_string = []\n",
    "    while i < len(df):\n",
    "        if df['Text'][i].find(word) != -1:\n",
    "            bad_string.append(df['Text'][i])\n",
    "        i += 1\n",
    "    for i in bad_string:\n",
    "        index_list_to_del = df.index[df['Text'] == i].tolist()\n",
    "        df = df.drop(index_list_to_del)\n",
    "        df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13122"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podslushano_labeled = pd.read_csv('podslushano_labeled.csv')\n",
    "podslushano_labeled = podslushano_labeled.drop('Unnamed: 0', axis=1)\n",
    "podslushano_labeled = podslushano_labeled.fillna(0)\n",
    "podslushano_labeled = podslushano_labeled.reset_index(drop=True)\n",
    "len(podslushano_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество деструктивных комментариев: 1978\n",
      "Количество недеструктивных комментариев: 11131\n"
     ]
    }
   ],
   "source": [
    "text_podslushano = np.array(podslushano_labeled['Text'].values)\n",
    "target_podslushano = podslushano_labeled['Target'].astype(int).values\n",
    "print('Количество деструктивных комментариев:', len(podslushano_labeled[podslushano_labeled['Target'] == 1]))\n",
    "print('Количество недеструктивных комментариев:', len(podslushano_labeled[podslushano_labeled['Target'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44208289054197664\n"
     ]
    }
   ],
   "source": [
    "print(processed_with_split(text_podslushano, target_podslushano))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
